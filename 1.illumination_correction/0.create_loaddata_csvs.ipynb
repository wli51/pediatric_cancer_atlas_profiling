{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LoadData CSVs to use for illumination correction\n",
    "\n",
    "In this notebook, we create a LoadData CSV that contains paths to each channel per image set for CellProfiler to process. \n",
    "We can use this LoadData CSV to run illumination correction (IC) pipeline that saves IC functions in `npy` file format and extract image quality metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import loaddata_utils as ld_utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for parameters to make loaddata csv\n",
    "index_directory = pathlib.Path(\"/media/18tbdrive/ALSF_pilot_data/SN0313537/\")\n",
    "config_path = pathlib.Path(\"./config.yml\").absolute()\n",
    "output_csv_dir = pathlib.Path(\"./loaddata_csvs\")\n",
    "output_csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Find all 'Images' folders within the directory\n",
    "images_folders = list(index_directory.rglob('Images'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LoadData CSVs for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK-N-MC_Re-imaged_BR00143976_loaddata_original.csv is created!\n",
      "SK-N-MC_Re-imaged_BR00143978_loaddata_original.csv is created!\n",
      "SK-N-MC_Re-imaged_BR00143980_loaddata_original.csv is created!\n",
      "SH-SY5Y_Re-imaged_BR00143976_loaddata_original.csv is created!\n",
      "SH-SY5Y_Re-imaged_BR00143978_loaddata_original.csv is created!\n",
      "SH-SY5Y_Re-imaged_BR00143980_loaddata_original.csv is created!\n",
      "BR00143979_loaddata_original.csv is created!\n",
      "A-673_Re-imaged_BR00143978_loaddata_original.csv is created!\n",
      "A-673_Re-imaged_BR00143980_loaddata_original.csv is created!\n",
      "A-673_Re-imaged_BR00143976_loaddata_original.csv is created!\n",
      "BR00143977_loaddata_original.csv is created!\n",
      "KP-N-YN_Re-imaged_BR00143979_loaddata_original.csv is created!\n",
      "KP-N-YN_Re-imaged_BR00143977_loaddata_original.csv is created!\n",
      "KP-N-YN_Re-imaged_BR00143981_loaddata_original.csv is created!\n",
      "BR00143980_loaddata_original.csv is created!\n",
      "SK-N-AS_Re-imaged_BR00143979_loaddata_original.csv is created!\n",
      "SK-N-AS_Re-imaged_BR00143981_loaddata_original.csv is created!\n",
      "SK-N-AS_Re-imaged_BR00143977_loaddata_original.csv is created!\n",
      "NB-1_Re-imaged_BR00143977_loaddata_original.csv is created!\n",
      "NB-1_Re-imaged_BR00143979_loaddata_original.csv is created!\n",
      "NB-1_Re-imaged_BR00143981_loaddata_original.csv is created!\n",
      "BR00143978_loaddata_original.csv is created!\n",
      "BR00143976_loaddata_original.csv is created!\n",
      "KNS-42_Re-imaged_BR00143979_loaddata_original.csv is created!\n",
      "KNS-42_Re-imaged_BR00143981_loaddata_original.csv is created!\n",
      "KNS-42_Re-imaged_BR00143977_loaddata_original.csv is created!\n",
      "CHP-212_Re-imaged_BR00143978_loaddata_original.csv is created!\n",
      "CHP-212_Re-imaged_BR00143976_loaddata_original.csv is created!\n",
      "CHP-212_Re-imaged_BR00143980_loaddata_original.csv is created!\n",
      "BR00143981_loaddata_original.csv is created!\n"
     ]
    }
   ],
   "source": [
    "# Loop through each folder and create a LoadData CSV\n",
    "for folder in images_folders:\n",
    "    # Get the first folder directly under the index_directory\n",
    "    relative_path = folder.relative_to(index_directory)\n",
    "    first_folder = relative_path.parts[0]  # First-level folder\n",
    "\n",
    "    # Generate the plate name based on folder structure\n",
    "    if first_folder.startswith('BR00'):\n",
    "        plate_name = first_folder.split('_')[0]  # Take the first part\n",
    "\n",
    "    elif first_folder.startswith('2024'):\n",
    "        second_folder = relative_path.parts[1]  # Second-level folder\n",
    "        part1 = '_'.join(first_folder.split('_')[-2:])  # Last two parts of first folder\n",
    "        part2 = second_folder.split('_')[0]  # First part of second folder\n",
    "        plate_name = f\"{part1}_{part2}\"  # Combine them\n",
    "\n",
    "    else:\n",
    "        print(f\"Unexpected folder pattern: {folder}\")\n",
    "        continue  # Skip if not matching patterns\n",
    "\n",
    "    # Create LoadData output path per plate\n",
    "    path_to_output_csv = (output_csv_dir / f\"{plate_name}_loaddata_original.csv\").absolute()\n",
    "\n",
    "    # Call the function to create the LoadData CSV\n",
    "    ld_utils.create_loaddata_csv(\n",
    "        index_directory=folder,\n",
    "        config_path=config_path,\n",
    "        path_to_output=path_to_output_csv,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat the re-imaged data back to their original plate and remove the original poor quality data paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect a list of original CSVs and identify unique plate IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 BR00 IDs: {'BR00143979', 'BR00143977', 'BR00143978', 'BR00143980', 'BR00143981', 'BR00143976'}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Find all CSV files in the output directory\n",
    "csv_files = list(output_csv_dir.glob(\"*.csv\"))\n",
    "\n",
    "# Step 2: Extract unique BR00 IDs from filenames\n",
    "br00_pattern = re.compile(r\"(BR00\\d+)\")  # Regex to match 'BR00' followed by digits\n",
    "\n",
    "# Collect all matching BR00 IDs from filenames\n",
    "br00_ids = {br00_pattern.search(csv_file.stem).group(1) \n",
    "            for csv_file in csv_files \n",
    "            if br00_pattern.search(csv_file.stem)}\n",
    "\n",
    "print(f\"Found {len(br00_ids)} BR00 IDs: {br00_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track/store files and add a metadata column for if a row is re-imaged or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example DataFrame for BR00 ID: BR00143979\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName_OrigBrightfield</th>\n",
       "      <th>PathName_OrigBrightfield</th>\n",
       "      <th>Metadata_Reimaged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r05c03f01p01-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/SN0313537/202...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r05c03f02p01-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/SN0313537/202...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r05c03f03p01-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/SN0313537/202...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r05c03f04p01-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/SN0313537/202...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r05c03f05p01-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/SN0313537/202...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>r12c12f05p01-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/SN0313537/202...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>r12c12f06p01-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/SN0313537/202...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>r12c12f07p01-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/SN0313537/202...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>r12c12f08p01-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/SN0313537/202...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>r12c12f09p01-ch1sk1fk1fl1.tiff</td>\n",
       "      <td>/media/18tbdrive/ALSF_pilot_data/SN0313537/202...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1977 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            FileName_OrigBrightfield  \\\n",
       "0     r05c03f01p01-ch1sk1fk1fl1.tiff   \n",
       "1     r05c03f02p01-ch1sk1fk1fl1.tiff   \n",
       "2     r05c03f03p01-ch1sk1fk1fl1.tiff   \n",
       "3     r05c03f04p01-ch1sk1fk1fl1.tiff   \n",
       "4     r05c03f05p01-ch1sk1fk1fl1.tiff   \n",
       "...                              ...   \n",
       "1972  r12c12f05p01-ch1sk1fk1fl1.tiff   \n",
       "1973  r12c12f06p01-ch1sk1fk1fl1.tiff   \n",
       "1974  r12c12f07p01-ch1sk1fk1fl1.tiff   \n",
       "1975  r12c12f08p01-ch1sk1fk1fl1.tiff   \n",
       "1976  r12c12f09p01-ch1sk1fk1fl1.tiff   \n",
       "\n",
       "                               PathName_OrigBrightfield  Metadata_Reimaged  \n",
       "0     /media/18tbdrive/ALSF_pilot_data/SN0313537/202...               True  \n",
       "1     /media/18tbdrive/ALSF_pilot_data/SN0313537/202...               True  \n",
       "2     /media/18tbdrive/ALSF_pilot_data/SN0313537/202...               True  \n",
       "3     /media/18tbdrive/ALSF_pilot_data/SN0313537/202...               True  \n",
       "4     /media/18tbdrive/ALSF_pilot_data/SN0313537/202...               True  \n",
       "...                                                 ...                ...  \n",
       "1972  /media/18tbdrive/ALSF_pilot_data/SN0313537/202...               True  \n",
       "1973  /media/18tbdrive/ALSF_pilot_data/SN0313537/202...               True  \n",
       "1974  /media/18tbdrive/ALSF_pilot_data/SN0313537/202...               True  \n",
       "1975  /media/18tbdrive/ALSF_pilot_data/SN0313537/202...               True  \n",
       "1976  /media/18tbdrive/ALSF_pilot_data/SN0313537/202...               True  \n",
       "\n",
       "[1977 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Initialize storage to track used files\n",
    "br00_dataframes = {br_id: [] for br_id in br00_ids}\n",
    "used_files = set()  # Store filenames used in the process\n",
    "concat_files = []  # Track new concatenated CSV files\n",
    "\n",
    "# Step 4: Add 'Metadata_Reimaged' column and group by BR00 ID\n",
    "for csv_file in csv_files:\n",
    "    filename = csv_file.stem\n",
    "    match = br00_pattern.search(filename)\n",
    "\n",
    "    if match:\n",
    "        br_id = match.group(1)\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        # Add 'Metadata_Reimaged' column based on filename\n",
    "        df['Metadata_Reimaged'] = 'Re-imaged' in filename\n",
    "\n",
    "        # Append the DataFrame to the corresponding BR00 group\n",
    "        br00_dataframes[br_id].append(df)\n",
    "\n",
    "        # Track this file as used\n",
    "        used_files.add(csv_file.name)\n",
    "\n",
    "# Print an example DataFrame (first BR00 group)\n",
    "example_id = next(iter(br00_dataframes))  # Get the first BR00 ID\n",
    "example_df = pd.concat(br00_dataframes[example_id], ignore_index=True)\n",
    "print(f\"\\nExample DataFrame for BR00 ID: {example_id}\")\n",
    "example_df.iloc[:, [0, 1, -1]]  # Display only the first two and last column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat the re-imaged and original data for the same plate and remove any duplicate wells that come from the original data\n",
    "\n",
    "We remove the duplicates that aren't re-imaged since they are of poor quality. We want to analyze the re-imaged data from those same wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: loaddata_csvs/BR00143979_concatenated.csv\n",
      "Saved: loaddata_csvs/BR00143977_concatenated.csv\n",
      "Saved: loaddata_csvs/BR00143978_concatenated.csv\n",
      "Saved: loaddata_csvs/BR00143980_concatenated.csv\n",
      "Saved: loaddata_csvs/BR00143981_concatenated.csv\n",
      "Saved: loaddata_csvs/BR00143976_concatenated.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Concatenate DataFrames, drop duplicates, and save per BR00 ID\n",
    "for br_id, dfs in br00_dataframes.items():\n",
    "    if dfs:  # Only process if there are matching files\n",
    "        concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Drop duplicates, prioritizing rows with 'Metadata_Reimaged' == True\n",
    "        deduplicated_df = concatenated_df.sort_values(\n",
    "            'Metadata_Reimaged', ascending=False\n",
    "        ).drop_duplicates(subset=['Metadata_Well', 'Metadata_Site'], keep='first')\n",
    "\n",
    "        # Sort by 'Metadata_Col' and 'Metadata_Row'\n",
    "        sorted_df = deduplicated_df.sort_values(\n",
    "            ['Metadata_Col', 'Metadata_Row'], ascending=True\n",
    "        )\n",
    "\n",
    "        # Save the cleaned, concatenated, and sorted DataFrame to a new CSV file\n",
    "        output_path = output_csv_dir / f\"{br_id}_concatenated.csv\"\n",
    "        sorted_df.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"Saved: {output_path}\")\n",
    "        concat_files.append(output_path)  # Track new concatenated files\n",
    "    else:\n",
    "        print(f\"No files found for {br_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm that all LoadData CSV files were included in previous concat (avoid data loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files were successfully used.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Verify all files were used\n",
    "unused_files = set(csv_file.name for csv_file in csv_files) - used_files\n",
    "\n",
    "if unused_files:\n",
    "    print(\"Warning: Some files were not used in the concatenation!\")\n",
    "    for file in unused_files:\n",
    "        print(f\"Unused: {file}\")\n",
    "else:\n",
    "    print(\"All files were successfully used.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the original CSV files to prevent CellProfiler from using them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: loaddata_csvs/NB-1_Re-imaged_BR00143979_loaddata_original.csv\n",
      "Removed: loaddata_csvs/SH-SY5Y_Re-imaged_BR00143976_loaddata_original.csv\n",
      "Removed: loaddata_csvs/SK-N-MC_Re-imaged_BR00143978_loaddata_original.csv\n",
      "Removed: loaddata_csvs/NB-1_Re-imaged_BR00143977_loaddata_original.csv\n",
      "Removed: loaddata_csvs/BR00143979_loaddata_original.csv\n",
      "Removed: loaddata_csvs/SH-SY5Y_Re-imaged_BR00143978_loaddata_original.csv\n",
      "Removed: loaddata_csvs/CHP-212_Re-imaged_BR00143976_loaddata_original.csv\n",
      "Removed: loaddata_csvs/KP-N-YN_Re-imaged_BR00143977_loaddata_original.csv\n",
      "Removed: loaddata_csvs/CHP-212_Re-imaged_BR00143980_loaddata_original.csv\n",
      "Removed: loaddata_csvs/KNS-42_Re-imaged_BR00143977_loaddata_original.csv\n",
      "Removed: loaddata_csvs/A-673_Re-imaged_BR00143980_loaddata_original.csv\n",
      "Removed: loaddata_csvs/BR00143977_loaddata_original.csv\n",
      "Removed: loaddata_csvs/KNS-42_Re-imaged_BR00143981_loaddata_original.csv\n",
      "Removed: loaddata_csvs/KNS-42_Re-imaged_BR00143979_loaddata_original.csv\n",
      "Removed: loaddata_csvs/SK-N-AS_Re-imaged_BR00143977_loaddata_original.csv\n",
      "Removed: loaddata_csvs/BR00143976_loaddata_original.csv\n",
      "Removed: loaddata_csvs/SK-N-MC_Re-imaged_BR00143976_loaddata_original.csv\n",
      "Removed: loaddata_csvs/NB-1_Re-imaged_BR00143981_loaddata_original.csv\n",
      "Removed: loaddata_csvs/KP-N-YN_Re-imaged_BR00143979_loaddata_original.csv\n",
      "Removed: loaddata_csvs/A-673_Re-imaged_BR00143978_loaddata_original.csv\n",
      "Removed: loaddata_csvs/CHP-212_Re-imaged_BR00143978_loaddata_original.csv\n",
      "Removed: loaddata_csvs/SH-SY5Y_Re-imaged_BR00143980_loaddata_original.csv\n",
      "Removed: loaddata_csvs/BR00143980_loaddata_original.csv\n",
      "Removed: loaddata_csvs/KP-N-YN_Re-imaged_BR00143981_loaddata_original.csv\n",
      "Removed: loaddata_csvs/BR00143978_loaddata_original.csv\n",
      "Removed: loaddata_csvs/A-673_Re-imaged_BR00143976_loaddata_original.csv\n",
      "Removed: loaddata_csvs/SK-N-MC_Re-imaged_BR00143980_loaddata_original.csv\n",
      "Removed: loaddata_csvs/SK-N-AS_Re-imaged_BR00143981_loaddata_original.csv\n",
      "Removed: loaddata_csvs/BR00143981_loaddata_original.csv\n",
      "Removed: loaddata_csvs/SK-N-AS_Re-imaged_BR00143979_loaddata_original.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Remove all non-concatenated CSVs to avoid confusion\n",
    "for csv_file in csv_files:\n",
    "    if csv_file not in concat_files:  # Keep only new concatenated files\n",
    "        csv_file.unlink()  # Delete the file\n",
    "        print(f\"Removed: {csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pe2loaddata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e682b7bd16cdbc4be2393bc1b1eed6b87cf8a0c86d477c0593cdffdecdf8222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
